# -*- coding: utf-8 -*-
"""Bias Mitigation (Adversarial debiasing).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZUt732I3PMDbsiTInehm7cLhvf9Gclme
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Imports
import os, random
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import LabelEncoder
from scipy.stats import entropy, wasserstein_distance

# Set reproducibility seed
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Paths
base_path = "/content/drive/MyDrive/"
input_folder = base_path + "CALMdatasets/"
truth_folder = base_path + "CALMtruthtable/"
output_folder = base_path + "Adversarial debiasing/"
metrics_folder = os.path.join(output_folder, "metrics/")
os.makedirs(metrics_folder, exist_ok=True)

class GradReverse(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, lambda_adv):
        ctx.lambda_adv = lambda_adv
        return x.clone()

    @staticmethod
    def backward(ctx, grad_output):
        return -ctx.lambda_adv * grad_output, None

def grad_reverse(x, lambda_adv):
    return GradReverse.apply(x, lambda_adv)

# Main Classifier Model
class MainModel(nn.Module):
    def __init__(self):
        super(MainModel, self).__init__()
        self.fc1 = nn.Linear(1, 16)
        self.fc2 = nn.Linear(16, 2)

    def forward(self, x):
        h = torch.relu(self.fc1(x))
        logits = self.fc2(h)
        return logits, h
class Adversary(nn.Module):
    def __init__(self, num_classes):
        super(Adversary, self).__init__()
        self.fc1 = nn.Linear(16, 16)
        self.fc2 = nn.Linear(16, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        logits = self.fc2(x)
        return logits

def adversarial_debiasing(df, score_col, label_col, sensitive_col, model_name, sensitive_name):
    le = LabelEncoder()
    df[sensitive_col + '_enc'] = le.fit_transform(df[sensitive_col])
    X = torch.tensor(df[[score_col]].values, dtype=torch.float32)
    Y = torch.tensor(df[label_col].values, dtype=torch.long)
    A = torch.tensor(df[sensitive_col + '_enc'].values, dtype=torch.long)
    dataset = TensorDataset(X, Y, A)
    loader = DataLoader(dataset, batch_size=32, shuffle=True)

    main_model = MainModel().to(device)
    adv_model = Adversary(len(le.classes_)).to(device)

    criterion = nn.CrossEntropyLoss()
    opt_main = optim.Adam(main_model.parameters(), lr=1e-3)
    opt_adv = optim.Adam(adv_model.parameters(), lr=1e-3)

    for epoch in range(10):
        lambda_adv = epoch / 10
        main_model.train(); adv_model.train()
        for Xb, Yb, Ab in loader:
            Xb, Yb, Ab = Xb.to(device), Yb.to(device), Ab.to(device)
            opt_main.zero_grad(); opt_adv.zero_grad()
            logits, hidden = main_model(Xb)
            loss_main = criterion(logits, Yb)
            hidden_rev = grad_reverse(hidden, lambda_adv)
            logits_adv = adv_model(hidden_rev.detach())
            loss_adv = criterion(logits_adv, Ab)
            (loss_main + loss_adv).backward()
            opt_main.step(); opt_adv.step()

    main_model.eval()
    with torch.no_grad():
        logits_all, _ = main_model(X.to(device))
        prob_positive = torch.softmax(logits_all, dim=1)[:, 1].cpu().numpy()

    df[f'debiased_{score_col}'] = prob_positive
    save_path = os.path.join(output_folder, f"{model_name}_{sensitive_name}_debiased.csv")
    df.to_csv(save_path, index=False)
    print(f"✅ Debiased results saved: {save_path}")
    return df, le

def compute_bias_metrics(df, sensitive_col, score_col, truth_label_col, threshold=0.5, bins=20, name=""):
    metrics = []
    df['binary'] = (df[score_col] >= threshold).astype(int)
    groups = df[sensitive_col].unique()

    def safe_metrics(df1, df2):
        if len(df1) == 0 or len(df2) == 0:
            return [np.nan] * 7
        spd_binary = df1['binary'].mean() - df2['binary'].mean()
        spd_cont = df1[score_col].mean() - df2[score_col].mean()
        hist1, _ = np.histogram(df1[score_col], bins=bins, range=(0, 1), density=True)
        hist2, _ = np.histogram(df2[score_col], bins=bins, range=(0, 1), density=True)
        hist1 += 1e-8
        hist2 += 1e-8
        jsd = entropy(hist1, hist2)
        wd = wasserstein_distance(df1[score_col], df2[score_col])
        tp1 = ((df1['binary'] == 1) & (df1[truth_label_col] == 1)).sum()
        fp1 = ((df1['binary'] == 1) & (df1[truth_label_col] == 0)).sum()
        fn1 = ((df1['binary'] == 0) & (df1[truth_label_col] == 1)).sum()
        tn1 = ((df1['binary'] == 0) & (df1[truth_label_col] == 0)).sum()
        tp2 = ((df2['binary'] == 1) & (df2[truth_label_col] == 1)).sum()
        fp2 = ((df2['binary'] == 1) & (df2[truth_label_col] == 0)).sum()
        fn2 = ((df2['binary'] == 0) & (df2[truth_label_col] == 1)).sum()
        tn2 = ((df2['binary'] == 0) & (df2[truth_label_col] == 0)).sum()
        eod = (tp1 / (tp1 + fn1 + 1e-8)) - (tp2 / (tp2 + fn2 + 1e-8))
        fpr = (fp1 / (fp1 + tn1 + 1e-8)) - (fp2 / (fp2 + tn2 + 1e-8))
        fnr = (fn1 / (fn1 + tp1 + 1e-8)) - (fn2 / (fn2 + tp2 + 1e-8))
        return [spd_binary, spd_cont, jsd, wd, eod, fpr, fnr]

    for g in groups:
        df_g = df[df[sensitive_col] == g]
        df_rest = df[df[sensitive_col] != g]
        metrics.append(["Group_vs_Rest", g] + safe_metrics(df_g, df_rest))

    for i, g1 in enumerate(groups):
        for g2 in groups[i+1:]:
            df1 = df[df[sensitive_col] == g1]
            df2 = df[df[sensitive_col] == g2]
            metrics.append(["Pairwise", f"{g1}_vs_{g2}"] + safe_metrics(df1, df2))

    binary_means = {g: df[df[sensitive_col] == g]['binary'].mean() for g in groups}
    max_g = max(binary_means, key=binary_means.get)
    min_g = min(binary_means, key=binary_means.get)
    df_max = df[df[sensitive_col] == max_g]
    df_min = df[df[sensitive_col] == min_g]
    metrics.append(["Max_vs_Min", f"{max_g}_vs_{min_g}"] + safe_metrics(df_max, df_min))

    cols = ['Type', 'Comparison', 'SPD_Binary', 'SPD_Continuous', 'JSD', 'WD', 'EOD', 'FPR', 'FNR']
    metrics_df = pd.DataFrame(metrics, columns=cols)
    metrics_file = os.path.join(metrics_folder, f"{name}_metrics.csv")
    metrics_df.to_csv(metrics_file, index=False)
    print(f"✅ Complete metrics saved: {metrics_file}")

# ✅ Run All 4 Configs: Roberta & MentalBERT × Gender & Race
for model in ['roberta', 'mentalbert']:
    for sensitive in ['gender', 'race']:
        pred_file = os.path.join(input_folder, f"{sensitive}_predictions.csv")
        truth_file = os.path.join(truth_folder, f"calm_truth_table_{sensitive}.csv")
        df_pred = pd.read_csv(pred_file)
        df_truth = pd.read_csv(truth_file)
        df_pred['truth_label'] = df_truth['truth_label']
        df_deb, _ = adversarial_debiasing(df_pred, f"{model}_score", "truth_label", sensitive, model, sensitive)
        compute_bias_metrics(df_deb, sensitive, f"debiased_{model}_score", "truth_label", name=f"{model}_{sensitive}")