# -*- coding: utf-8 -*-
"""FineTune RoBERTa and MentalBERT on CALM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/161n4ttkBvjxIeJd4AQWfQFRcNPESvIVq
"""

!pip install datasets

import os
import json
import random
import pandas as pd
import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset, Value
from sklearn.model_selection import train_test_split
from huggingface_hub import login

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define save directory in Google Drive
save_dir = "/content/drive/MyDrive/approach2"
os.makedirs(save_dir, exist_ok=True)

# ============================================
# Section 1: Environment Setup & Loading CALM Datasets
# ============================================
print("Downloading and Loading CALM Datasets...")

# Create necessary directories for data
os.makedirs("CALM/data/gender_datasets", exist_ok=True)
os.makedirs("CALM/data/race_datasets", exist_ok=True)

# Download Gender and Race datasets
!wget -O CALM/data/gender_datasets/sentiment_gender_dataset.jsonl "https://huggingface.co/datasets/vipulgupta/CALM/raw/main/data/gender_datasets/sentiment_gender_dataset.jsonl"
!wget -O CALM/data/race_datasets/sentiment_race_dataset.jsonl "https://huggingface.co/datasets/vipulgupta/CALM/raw/main/data/race_datasets/sentiment_race_dataset.jsonl"

print("Gender dataset exists:", os.path.exists("CALM/data/gender_datasets/sentiment_gender_dataset.jsonl"))
print("Race dataset exists:", os.path.exists("CALM/data/race_datasets/sentiment_race_dataset.jsonl"))

# Load the Gender and Race datasets into DataFrames
gender_data_path = "CALM/data/gender_datasets/sentiment_gender_dataset.jsonl"
race_data_path   = "CALM/data/race_datasets/sentiment_race_dataset.jsonl"

with open(gender_data_path, "r", encoding="utf-8") as f:
    gender_data = [json.loads(line) for line in f]
df_calm_gender = pd.DataFrame(gender_data)[["sentence", "gender"]].rename(columns={"sentence": "text"})

with open(race_data_path, "r", encoding="utf-8") as f:
    race_data = [json.loads(line) for line in f]
df_calm_race = pd.DataFrame(race_data)[["sentence", "race"]].rename(columns={"sentence": "text"})
# Check unique values before applying mapping
print("Before Mapping - Unique Gender Labels:", df_calm_gender["gender"].unique())
print("Before Mapping - Unique Race Labels:", df_calm_race["race"].unique())

# Ensure labels are lowercase before mapping
df_calm_gender["gender"] = df_calm_gender["gender"].str.lower()
df_calm_race["race"] = df_calm_race["race"].str.lower()

# Corrected mapping with lowercase keys
gender_mapping = {'male': 0, 'female': 1, 'non-binary': 2}
df_calm_gender["gender"] = df_calm_gender["gender"].map(gender_mapping)

race_mapping = {'white': 0, 'black': 1, 'asian': 2, 'hispanic': 3}
df_calm_race["race"] = df_calm_race["race"].map(race_mapping)

# Check if labels are now mapped correctly
print("After Fix - Gender Label Distribution:\n", df_calm_gender["gender"].value_counts())
print("After Fix - Race Label Distribution:\n", df_calm_race["race"].value_counts())

# ============================================
# Section 2: Tokenization and Dataset Conversion
# ============================================
print("Tokenizing dataset...")

# Authenticate Hugging Face API Token
#login(token="hf_..........................")

# Load Tokenizers
tokenizer_roberta = RobertaTokenizer.from_pretrained("roberta-base")
tokenizer_bert = BertTokenizer.from_pretrained("mental/mental-bert-base-uncased", use_fast=True)

# Tokenization function
def tokenize_function(examples, tokenizer):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Convert Pandas DataFrames to Hugging Face Dataset
def convert_to_dataset(df, label_col, tokenizer):
    dataset = Dataset.from_pandas(df)
    dataset = dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)
    dataset = dataset.rename_column(label_col, "labels")
    # Convert labels to plain integers for correct type and shape
    dataset = dataset.map(lambda x: {"labels": int(x["labels"]) if x["labels"] is not None else 0}, batched=False)
    # Explicitly cast the labels column to int64 using Value
    dataset = dataset.cast_column("labels", Value("int64"))
    dataset = dataset.remove_columns([col for col in dataset.column_names if col not in ["input_ids", "attention_mask", "labels"]])
    return dataset

gender_train, gender_test = train_test_split(df_calm_gender, test_size=0.1, random_state=42)
race_train, race_test = train_test_split(df_calm_race, test_size=0.1, random_state=42)

gender_train_dataset_roberta = convert_to_dataset(gender_train, "gender", tokenizer_roberta)
gender_test_dataset_roberta = convert_to_dataset(gender_test, "gender", tokenizer_roberta)
race_train_dataset_roberta = convert_to_dataset(race_train, "race", tokenizer_roberta)
race_test_dataset_roberta = convert_to_dataset(race_test, "race", tokenizer_roberta)

gender_train_dataset_bert = convert_to_dataset(gender_train, "gender", tokenizer_bert)
gender_test_dataset_bert = convert_to_dataset(gender_test, "gender", tokenizer_bert)
race_train_dataset_bert = convert_to_dataset(race_train, "race", tokenizer_bert)
race_test_dataset_bert = convert_to_dataset(race_test, "race", tokenizer_bert)

print("Unique gender labels after dataset conversion:", set(gender_train_dataset_roberta["labels"]))
print("Unique race labels after dataset conversion:", set(race_train_dataset_roberta["labels"]))

# ============================================
# Section 3: Model Training Function
# ============================================
def train_model(model_name, tokenizer, train_dataset, test_dataset, num_labels, output_dir, num_epochs, model_type):
    if model_type == "roberta":
        model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
        # Set problem type to single_label_classification to use CrossEntropyLoss correctly
        model.config.problem_type = "single_label_classification"
        model.to(device)
    elif model_type == "bert":
        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
        model.config.problem_type = "single_label_classification"
        model.to(device)
    else:
        raise ValueError("Unsupported model type. Use 'roberta' or 'bert'.")

    training_args = TrainingArguments(
        report_to=["none"],
        output_dir=output_dir,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=num_epochs,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=500,
        save_total_limit=2,
        push_to_hub=False,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
    )

    trainer.train()
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    return model

from collections import Counter

# Print unique labels
print("Unique gender labels in training set:", set(gender_train_dataset_roberta["labels"]))
print("Unique race labels in training set:", set(race_train_dataset_roberta["labels"]))

# Print label distribution
print("Gender label distribution:", Counter(gender_train_dataset_roberta["labels"]))
print("Race label distribution:", Counter(race_train_dataset_roberta["labels"]))

# ============================================
# Section 4: Train and Save RoBERTa Models
# ============================================
print("Training RoBERTa for Gender Classification...")
gender_model_roberta = train_model("roberta-base", tokenizer_roberta, gender_train_dataset_roberta, gender_test_dataset_roberta, 3, f"{save_dir}/roberta/gender", 3, "roberta")

print("Training RoBERTa for Race Classification...")
race_model_roberta = train_model("roberta-base", tokenizer_roberta, race_train_dataset_roberta, race_test_dataset_roberta, 4, f"{save_dir}/roberta/race", 3, "roberta")

# ============================================
# Section 5: Train and Save MentalBERT Models
# ============================================
print("Training MentalBERT for Gender Classification...")
gender_model_bert = train_model("mental/mental-bert-base-uncased", tokenizer_bert, gender_train_dataset_bert, gender_test_dataset_bert, 3, f"{save_dir}/mentalbert/gender", 5, "bert")

print("Training MentalBERT for Race Classification...")
race_model_bert = train_model("mental/mental-bert-base-uncased", tokenizer_bert, race_train_dataset_bert, race_test_dataset_bert, 4, f"{save_dir}/mentalbert/race", 5, "bert")

print("Training Complete! Models Saved to Google Drive.")

import os

save_dir = "/content/drive/MyDrive/approach2/"

print("RoBERTa Gender Model Exists:", os.path.exists(save_dir + "roberta/gender/pytorch_model.bin"))
print("RoBERTa Race Model Exists:", os.path.exists(save_dir + "roberta/race/pytorch_model.bin"))
print("MentalBERT Gender Model Exists:", os.path.exists(save_dir + "mentalbert/gender/pytorch_model.bin"))
print("MentalBERT Race Model Exists:", os.path.exists(save_dir + "mentalbert/race/pytorch_model.bin"))

# Manually re-save the trained models
models = {
    "roberta_gender": gender_model_roberta,
    "roberta_race": race_model_roberta,
    "mentalbert_gender": gender_model_bert,
    "mentalbert_race": race_model_bert
}

save_dir = "/content/drive/MyDrive/approach2/"

for model_name, model in models.items():
    model_path = os.path.join(save_dir, model_name)
    os.makedirs(model_path, exist_ok=True)
    print(f"Saving {model_name} to {model_path}...")
    model.save_pretrained(model_path)

print("✅ All models saved successfully!")

import os

save_dir = "/content/drive/MyDrive/approach2"
os.makedirs(save_dir, exist_ok=True)
os.makedirs(save_dir + "/roberta/gender", exist_ok=True)
os.makedirs(save_dir + "/roberta/race", exist_ok=True)
os.makedirs(save_dir + "/mentalbert/gender", exist_ok=True)
os.makedirs(save_dir + "/mentalbert/race", exist_ok=True)

print("✅ Directories created successfully!")

# Manually re-save the trained models
models = {
    "roberta/gender": gender_model_roberta,
    "roberta/race": race_model_roberta,
    "mentalbert/gender": gender_model_bert,
    "mentalbert/race": race_model_bert
}

for model_name, model in models.items():
    model_path = os.path.join(save_dir, model_name)
    os.makedirs(model_path, exist_ok=True)  # Ensure directory exists
    print(f"Saving {model_name} to {model_path}...")

    # Save model and tokenizer
    model.save_pretrained(model_path)
    tokenizer_roberta.save_pretrained(model_path) if "roberta" in model_name else tokenizer_bert.save_pretrained(model_path)

print("✅ All models saved successfully!")

print("RoBERTa Gender Model:", gender_model_roberta)
print("RoBERTa Race Model:", race_model_roberta)
print("MentalBERT Gender Model:", gender_model_bert)
print("MentalBERT Race Model:", race_model_bert)

import os

# Define model paths
model_paths = {
    "roberta/gender": gender_model_roberta,
    "roberta/race": race_model_roberta,
    "mentalbert/gender": gender_model_bert,
    "mentalbert/race": race_model_bert
}

# Ensure the save directory exists
os.makedirs(save_dir, exist_ok=True)

for model_name, model in model_paths.items():
    model_path = os.path.join(save_dir, model_name)
    os.makedirs(model_path, exist_ok=True)  # Ensure directory exists

    if model is None:
        print(f"❌ {model_name} model is None. Skipping save.")
        continue

    print(f"Saving {model_name} to {model_path}...")

    try:
        # Save model and tokenizer
        model.save_pretrained(model_path)
        if "roberta" in model_name:
            tokenizer_roberta.save_pretrained(model_path)
        else:
            tokenizer_bert.save_pretrained(model_path)
        print(f"✅ {model_name} saved successfully!")
    except Exception as e:
        print(f"❌ Error saving {model_name}: {e}")

import os

print("RoBERTa Gender Model Exists:", os.path.exists(save_dir + "/roberta/gender/pytorch_model.bin"))
print("RoBERTa Race Model Exists:", os.path.exists(save_dir + "/roberta/race/pytorch_model.bin"))
print("MentalBERT Gender Model Exists:", os.path.exists(save_dir + "/mentalbert/gender/pytorch_model.bin"))
print("MentalBERT Race Model Exists:", os.path.exists(save_dir + "/mentalbert/race/pytorch_model.bin"))

import os

# Define save directory
save_dir = "/content/drive/MyDrive/approach2"
os.makedirs(save_dir, exist_ok=True)

# Define paths
model_paths = {
    "roberta/gender": gender_model_roberta,
    "roberta/race": race_model_roberta,
    "mentalbert/gender": gender_model_bert,
    "mentalbert/race": race_model_bert
}

for model_name, model in model_paths.items():
    model_path = os.path.join(save_dir, model_name)
    os.makedirs(model_path, exist_ok=True)

    if model is None:
        print(f"❌ {model_name} model is None. Skipping save.")
        continue

    print(f"Saving {model_name} to {model_path}...")

    try:
        # Save model and tokenizer
        model.save_pretrained(model_path)
        if "roberta" in model_name:
            tokenizer_roberta.save_pretrained(model_path)
        else:
            tokenizer_bert.save_pretrained(model_path)
        print(f"✅ {model_name} saved successfully!")
    except Exception as e:
        print(f"❌ Error saving {model_name}: {e}")

import os

print("RoBERTa Gender Model Exists:", os.path.exists(save_dir + "/roberta/gender/pytorch_model.bin"))
print("RoBERTa Race Model Exists:", os.path.exists(save_dir + "/roberta/race/pytorch_model.bin"))
print("MentalBERT Gender Model Exists:", os.path.exists(save_dir + "/mentalbert/gender/pytorch_model.bin"))
print("MentalBERT Race Model Exists:", os.path.exists(save_dir + "/mentalbert/race/pytorch_model.bin"))

from google.colab import drive
drive.mount('/content/mydrive')

print("Google Drive Accessible:", os.path.exists("/content/drive/MyDrive"))
print("Approach2 Folder Exists:", os.path.exists(save_dir))

import os

save_dir = "/content/drive/MyDrive/approach2"

print("Files inside Approach2 directory:")
print(os.listdir(save_dir))

# Check subdirectories for models
for subdir in ["roberta/gender", "roberta/race", "mentalbert/gender", "mentalbert/race"]:
    model_path = os.path.join(save_dir, subdir)
    if os.path.exists(model_path):
        print(f"✅ {subdir} exists. Files inside: {os.listdir(model_path)}")
    else:
        print(f"❌ {subdir} does NOT exist!")

import os

def check_model_exists(model_dir):
    return os.path.exists(os.path.join(save_dir, model_dir, "model.safetensors"))

print("RoBERTa Gender Model Exists:", check_model_exists("roberta/gender"))
print("RoBERTa Race Model Exists:", check_model_exists("roberta/race"))
print("MentalBERT Gender Model Exists:", check_model_exists("mentalbert/gender"))
print("MentalBERT Race Model Exists:", check_model_exists("mentalbert/race"))

from google.colab import drive
import os

drive.mount('/content/MyDrive')

# Verify if the save directory exists
save_dir = "/content/drive/MyDrive/approach2"
print("Google Drive Accessible:", os.path.exists("/content/drive/MyDrive"))
print("Approach2 Folder Exists:", os.path.exists(save_dir))

import os

print("Google Drive Mounted:", os.path.exists("/content/drive"))
print("MyDrive Exists:", os.path.exists("/content/drive/MyDrive"))

import os

save_dir = "/content/drive/MyDrive/approach2"

print("Approach2 Directory Exists:", os.path.exists(save_dir))
print("Files in Approach2:", os.listdir(save_dir) if os.path.exists(save_dir) else "Directory not found")

print("Files in Colab temporary storage:", os.listdir("/content/"))

!ls -lh "/content/drive/MyDrive/approach2"

!chmod -R 777 /content/drive/MyDrive/approach2

!zip -r models_backup.zip /content/drive/MyDrive/approach2

import os
print("Files in Approach2:", os.listdir("/content/drive/MyDrive/approach2") if os.path.exists("/content/drive/MyDrive/approach2") else "Directory not found")

!mv /content/drive/MyDrive/approach2 /content/drive/MyDrive/approach2_backup

!cp -r /content/drive/MyDrive/approach2 /content/drive/MyDrive/approach2_copy

import os
print("Files in MyDrive:", os.listdir("/content/drive/MyDrive"))

!mv /content/drive/MyDrive/approach2_backup /content/drive/MyDrive/approach2

from google.colab import drive
drive.flush_and_unmount()
drive.mount('/content/MyDrive')

import os
print("Files in MyDrive:", os.listdir("/content/drive/MyDrive"))

import os
print("Files inside Approach2:", os.listdir("/content/drive/MyDrive/approach2"))

!zip -r /content/drive/MyDrive/approach2_backup.zip /content/drive/MyDrive/approach2

import os
print("Files in MyDrive:", os.listdir("/content/drive/MyDrive"))

from google.colab import files
files.download("/content/drive/MyDrive/approach2_backup.zip")
