# -*- coding: utf-8 -*-
"""Fine-Tuning MentalBERT on SST-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vZfDcJMMzsSSCH5hiQBYwDoShbdASY2t
"""

# ===============================
# ðŸ“Œ STEP 1: Install Required Libraries
# ===============================
!pip install datasets transformers torch torchvision torchaudio

# ===============================
# ðŸ“Œ STEP 2: Import Required Libraries
# ===============================
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
import torch
from sklearn.metrics import accuracy_score, f1_score
import os

# =========================================
# ðŸ“Œ STEP 3: Securely Set Hugging Face API Token
# =========================================
#token = "hf_RU...................."

# =========================================
# ðŸ“Œ STEP 4: Check GPU Availability & Set Device
# =========================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… Using {device} for processing!")

# =========================================
# ðŸ“Œ STEP 5: Load SST-2 Dataset
# =========================================
raw_datasets = load_dataset("glue", "sst2")

# âœ… Ensure only valid labels (0 = Negative, 1 = Positive)
raw_datasets = raw_datasets.filter(lambda x: x["label"] in [0, 1])
print("Unique labels in dataset:", set(raw_datasets["train"]["label"]))

# =========================================
# ðŸ“Œ STEP 6: Initialize MentalBERT Tokenizer
# =========================================
tokenizer = AutoTokenizer.from_pretrained(
    "mental/mental-bert-base-uncased",
    token=token,
    use_fast=True
)

# =========================================
# ðŸ“Œ STEP 7: Tokenize the Dataset (Using GPU)
# =========================================
def tokenize_function(examples):
    return tokenizer(
        examples["sentence"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

tokenized_datasets = raw_datasets.map(
    tokenize_function,
    batched=True,
    remove_columns=["sentence", "idx"]
)

# =========================================
# ðŸ“Œ STEP 8: Rename "label" to "labels"
# =========================================
tokenized_datasets = tokenized_datasets.map(lambda x: {"labels": x["label"]}, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(["label"])
tokenized_datasets.set_format("torch")

print(" Dataset Tokenized Successfully!")

# =========================================
# ðŸ“Œ STEP 9: Load Pretrained MentalBERT Model (On GPU)
# =========================================
model = AutoModelForSequenceClassification.from_pretrained(
    "mental/mental-bert-base-uncased",
    token=token,
    num_labels=2
).to(device)

# =========================================
# ðŸ“Œ STEP 10: Configure Training Arguments
# =========================================
training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/trained_models/mentalbert_sst2",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=16,  # âœ… Fixed for GPU
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    learning_rate=1e-5,
    weight_decay=0.1,
    logging_dir="/content/drive/MyDrive/trained_models/logs",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    fp16=True,  # âœ… Mixed Precision enabled for GPU
    report_to=["none"],
)

# =========================================
# ðŸ“Œ STEP 11: Define Evaluation Metrics
# =========================================
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="weighted")
    }

# =========================================
# ðŸ“Œ STEP 12: Initialize Trainer
# =========================================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# =========================================
# ðŸ“Œ STEP 13: Train the Model (On GPU)
# =========================================
trainer.train()

# =========================================
# ðŸ“Œ STEP 14: Save Trained Model to Google Drive
# =========================================
from google.colab import drive
drive.mount('/content/drive')

model.save_pretrained("/content/drive/MyDrive/trained_models/mentalbert_sst2")
tokenizer.save_pretrained("/content/drive/MyDrive/trained_models/mentalbert_sst2")

print(" Model successfully trained and saved to Google Drive!")
