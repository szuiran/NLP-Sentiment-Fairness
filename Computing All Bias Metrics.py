# -*- coding: utf-8 -*-
"""AllBiasMetrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k7Y12LueOwwo5VZG6Fhz77BWCiwZSsVT
"""

# ‚úÖ Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# üìÅ Define base path
base_path = "/content/drive/MyDrive"

# üìÑ File paths
gender_pred_path = f"{base_path}/CALMdatasets/gender_predictions.csv"
race_pred_path = f"{base_path}/CALMdatasets/race_predictions.csv"
gender_truth_path = f"{base_path}/CALMtruthtable/calm_truth_table_gender.csv"
race_truth_path = f"{base_path}/CALMtruthtable/calm_truth_table_race.csv"

# üì• Load CSVs
import pandas as pd
gender_pred = pd.read_csv(gender_pred_path)
race_pred = pd.read_csv(race_pred_path)
gender_truth = pd.read_csv(gender_truth_path)
race_truth = pd.read_csv(race_truth_path)

# üîó Merge with truth labels
df_gender = gender_pred.merge(gender_truth[['text', 'truth_label']], on='text')
df_race = race_pred.merge(race_truth[['text', 'truth_label']], on='text')

# üìä Bias metrics helpers
import numpy as np
from scipy.stats import wasserstein_distance
from scipy.spatial.distance import jensenshannon
import os

def compute_group_distributions_binary(df, label_col, group_col):
    distributions = {}
    for group in df[group_col].unique():
        group_data = df[df[group_col] == group][label_col]
        p0 = np.mean(group_data == 0)
        p1 = np.mean(group_data == 1)
        distributions[group] = np.array([p0, p1])
    return distributions

def compute_jsd_wd(distributions):
    results = {}
    groups = list(distributions.keys())
    for i in range(len(groups)):
        for j in range(i+1, len(groups)):
            g1, g2 = groups[i], groups[j]
            d1, d2 = distributions[g1], distributions[g2]
            jsd = jensenshannon(d1, d2)
            wd = wasserstein_distance([0, 1], [0, 1], u_weights=d1, v_weights=d2)
            key = f"{g1}_vs_{g2}"
            results[key] = {"JSD": jsd, "WD": wd}
    return results

# üìå Full bias metrics function
def generate_bias_metrics_format(df, model_name, group_col, label_col, score_col, output_name):
    df = df.copy()
    df['binary'] = df[label_col]
    groups = df[group_col].unique()
    results = []

    def get_rates(d):
        tp = ((d['binary'] == 1) & (d['truth_label'] == 1)).sum()
        fn = ((d['binary'] == 0) & (d['truth_label'] == 1)).sum()
        fp = ((d['binary'] == 1) & (d['truth_label'] == 0)).sum()
        tn = ((d['binary'] == 0) & (d['truth_label'] == 0)).sum()
        tpr = tp / (tp + fn + 1e-8)
        fpr = fp / (fp + tn + 1e-8)
        fnr = fn / (fn + tp + 1e-8)
        return tpr, fpr, fnr

    # --- Group vs Rest ---
    for g in groups:
        df_g = df[df[group_col] == g]
        df_rest = df[df[group_col] != g]

        spd_binary = df_g['binary'].mean() - df_rest['binary'].mean()
        spd_cont = df_g[score_col].mean() - df_rest[score_col].mean()

        tpr_g, fpr_g, fnr_g = get_rates(df_g)
        tpr_r, fpr_r, fnr_r = get_rates(df_rest)
        eod = tpr_g - tpr_r
        fpr_diff = fpr_g - fpr_r
        fnr_diff = fnr_g - fnr_r

        dist = compute_group_distributions_binary(df, label_col, group_col)
        jsd_wd = compute_jsd_wd(dist)
        jsd_vals = [val["JSD"] for key, val in jsd_wd.items() if g in key]
        wd_vals = [val["WD"] for key, val in jsd_wd.items() if g in key]
        avg_jsd = np.mean(jsd_vals) if jsd_vals else np.nan
        avg_wd = np.mean(wd_vals) if wd_vals else np.nan

        results.append(["Group_vs_Rest", g, spd_binary, spd_cont, avg_jsd, avg_wd, eod, fpr_diff, fnr_diff])

    # --- Pairwise ---
    for i, g1 in enumerate(groups):
        for g2 in groups[i+1:]:
            df1 = df[df[group_col] == g1]
            df2 = df[df[group_col] == g2]

            spd_binary = df1['binary'].mean() - df2['binary'].mean()
            spd_cont = df1[score_col].mean() - df2[score_col].mean()

            tpr1, fpr1, fnr1 = get_rates(df1)
            tpr2, fpr2, fnr2 = get_rates(df2)
            eod = tpr1 - tpr2
            fpr_diff = fpr1 - fpr2
            fnr_diff = fnr1 - fnr2

            dist = compute_group_distributions_binary(df, label_col, group_col)
            jsd = compute_jsd_wd(dist).get(f"{g1}_vs_{g2}", {}).get("JSD",
                  compute_jsd_wd(dist).get(f"{g2}_vs_{g1}", {}).get("JSD", np.nan))
            wd = compute_jsd_wd(dist).get(f"{g1}_vs_{g2}", {}).get("WD",
                 compute_jsd_wd(dist).get(f"{g2}_vs_{g1}", {}).get("WD", np.nan))

            results.append(["Pairwise", f"{g1}_vs_{g2}", spd_binary, spd_cont, jsd, wd, eod, fpr_diff, fnr_diff])

    # --- Max vs Min ---
    binary_means = df.groupby(group_col)['binary'].mean()
    max_group = binary_means.idxmax()
    min_group = binary_means.idxmin()
    df_max = df[df[group_col] == max_group]
    df_min = df[df[group_col] == min_group]

    spd_binary = df_max['binary'].mean() - df_min['binary'].mean()
    spd_cont = df_max[score_col].mean() - df_min[score_col].mean()

    tpr1, fpr1, fnr1 = get_rates(df_max)
    tpr2, fpr2, fnr2 = get_rates(df_min)
    eod = tpr1 - tpr2
    fpr_diff = fpr1 - fpr2
    fnr_diff = fnr1 - fnr2

    dist = compute_group_distributions_binary(df, label_col, group_col)
    key1 = f"{max_group}_vs_{min_group}"
    key2 = f"{min_group}_vs_{max_group}"
    jsd = compute_jsd_wd(dist).get(key1, {}).get("JSD", compute_jsd_wd(dist).get(key2, {}).get("JSD", np.nan))
    wd = compute_jsd_wd(dist).get(key1, {}).get("WD", compute_jsd_wd(dist).get(key2, {}).get("WD", np.nan))

    results.append(["Max_vs_Min", f"{max_group}_vs_{min_group}", spd_binary, spd_cont, jsd, wd, eod, fpr_diff, fnr_diff])

    # ‚úÖ Save results
    output_folder = f"{base_path}/CALMresults/AllBiasMetrics"
    os.makedirs(output_folder, exist_ok=True)

    df_results = pd.DataFrame(results, columns=[
        "Type", "Comparison", "SPD_Binary", "SPD_Continuous", "JSD", "WD", "EOD", "FPR", "FNR"
    ])
    df_results.to_csv(os.path.join(output_folder, f"{output_name}.csv"), index=False)
    print(f"‚úÖ Saved: {output_name}.csv in {output_folder}")

# ‚úÖ Run for all
generate_bias_metrics_format(df_gender, "mentalbert", "gender", "mentalbert_label", "mentalbert_score", "mentalbert_gender_metrics")
generate_bias_metrics_format(df_race, "mentalbert", "race", "mentalbert_label", "mentalbert_score", "mentalbert_race_metrics")
generate_bias_metrics_format(df_gender, "roberta", "gender", "roberta_label", "roberta_score", "roberta_gender_metrics")
generate_bias_metrics_format(df_race, "roberta", "race", "roberta_label", "roberta_score", "roberta_race_metrics")