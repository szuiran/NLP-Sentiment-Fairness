# -*- coding: utf-8 -*-
"""1-Fine-Tuning RoBERTa on SST-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IZLYPR0UO6k3j5M13VQ0mLZipGddCrmc
"""

# Step 1: Import Necessary Libraries
# Install missing libraries if needed
!pip install datasets transformers

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
import torch
from sklearn.metrics import accuracy_score, f1_score
import os

# Step 2: Mount Google Drive to Save Model
from google.colab import drive
drive.mount('/content/drive')

# Define model save path
drive_model_path = "/content/drive/MyDrive/trained_models/roberta_sst2"

# Step 3: Load the SST-2 dataset
raw_datasets = load_dataset("glue", "sst2")

# Check dataset structure
print("Train columns:", raw_datasets["train"].column_names)
print("Validation columns:", raw_datasets["validation"].column_names)
print("Test columns:", raw_datasets["test"].column_names)

# Step 4: Tokenization
tokenizer = AutoTokenizer.from_pretrained("roberta-base", use_fast=True)

def tokenize_function(examples):
    return tokenizer(examples["sentence"], padding="max_length", truncation=True, max_length=128)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=["sentence", "idx"])

# Rename "label" column to "labels" for Trainer compatibility
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")

# Step 5: Load Pretrained RoBERTa Model
model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=2)

# Step 6: Start with CPU for Setup
device = torch.device("cpu")
model.to(device)
print(f" Model is initially loaded on {device}")

# Step 7: Configure Training Arguments (Handles Both CPU & GPU)
training_args = TrainingArguments(
    output_dir=drive_model_path,  # Save directly to Drive
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=16 if torch.cuda.is_available() else 8,  # Auto-adjust batch size
    per_device_eval_batch_size=16 if torch.cuda.is_available() else 8,
    num_train_epochs=3,
    learning_rate=1e-5,
    weight_decay=0.1,
    logging_dir="/content/drive/MyDrive/trained_models/logs",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    fp16=torch.cuda.is_available(),
    logging_steps=50,
    report_to=["none"],
)

# Step 8: Switch to GPU Before Training (If Available)
if torch.cuda.is_available():
    device = torch.device("cuda")
    model.to(device)
    print(f" Model switched to {device} for training")

# Step 9: Define Trainer and Train Model
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="weighted")
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

# Step 10: Save Trained Model to Google Drive
model.save_pretrained(drive_model_path)
tokenizer.save_pretrained(drive_model_path)

print(" Model successfully trained and saved to Google Drive!")

# Mount Google Drive (if not mounted)
from google.colab import drive
drive.mount("/content/drive")

# Import necessary libraries
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

# Define the local path
roberta_model_path = "/content/drive/MyDrive/trained_models/roberta_sst2"

tokenizer = AutoTokenizer.from_pretrained(roberta_model_path, local_files_only=True)
model = AutoModelForSequenceClassification.from_pretrained(roberta_model_path, local_files_only=True)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print(" RoBERTa Model & Tokenizer Loaded Successfully!")

# =========================================
#  Restore Trainer Before Evaluation
# =========================================

# Re-import necessary libraries
from transformers import Trainer, TrainingArguments
import torch
from sklearn.metrics import accuracy_score, f1_score

# âœ… Ensure `roberta_model_path` is correctly defined
roberta_model_path = "/content/drive/MyDrive/trained_models/roberta_sst2"

# =========================================
#  STEP 2: Install Required Libraries (If Needed)
# =========================================
!pip install datasets transformers scikit-learn pandas

# =========================================
#  STEP 3: Import Necessary Libraries
# =========================================
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
import torch
from sklearn.metrics import accuracy_score, f1_score
import pandas as pd
import json

# =========================================
#  STEP 4: Load the Trained RoBERTa Model & Tokenizer
# =========================================
roberta_model_path = "/content/drive/MyDrive/trained_models/roberta_sst2"

# âœ… Load tokenizer and model from Google Drive
tokenizer = AutoTokenizer.from_pretrained(roberta_model_path, local_files_only=True)
model = AutoModelForSequenceClassification.from_pretrained(roberta_model_path, local_files_only=True)

#  STEP 5: Reload Dataset and Tokenization
# =========================================
raw_datasets = load_dataset("glue", "sst2")

def tokenize_function(examples):
    return tokenizer(examples["sentence"], padding="max_length", truncation=True, max_length=128)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=["sentence", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")

# =========================================
#  STEP 6: Define Compute Metrics Function
# =========================================
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="weighted")
    }

# =========================================
#  STEP 7: Reinitialize Trainer
# =========================================
training_args = TrainingArguments(
    output_dir=roberta_model_path,
    per_device_eval_batch_size=16 if torch.cuda.is_available() else 8,
    report_to=["none"]
)

trainer = Trainer(
    model=model,
    args=training_args,
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print(" Trainer Reinitialized")

# =========================================
# ðŸ“Œ STEP 8: Evaluate the Model on Validation Set
# =========================================
val_metrics = trainer.evaluate(eval_dataset=tokenized_datasets["validation"])
print(" Validation Metrics:", val_metrics)

#  Save validation metrics to Google Drive
with open(f"{roberta_model_path}/validation_metrics.json", "w") as f:
    json.dump(val_metrics, f)

print(" Validation Metrics Saved!")

# ðŸ“Œ STEP 9: Perform Error Analysis
# =========================================
predictions = trainer.predict(tokenized_datasets["validation"])
logits = predictions.predictions
true_labels = predictions.label_ids
predicted_labels = logits.argmax(axis=1)

df = pd.DataFrame({"True Label": true_labels, "Predicted Label": predicted_labels})
df.to_csv(f"{roberta_model_path}/error_analysis.csv", index=False)

print(" Error Analysis Saved to Google Drive!")